import datetime
import os
import tempfile
from pathlib import Path

import openai
import streamlit as st
from langchain.callbacks.base import BaseCallbackManager
from langchain.callbacks.streamlit import StreamlitCallbackHandler
from langchain.chat_models import ChatOpenAI
from llama_index import (
    GPTVectorStoreIndex,
    PromptHelper,
    ServiceContext,
    SimpleDirectoryReader,
    StorageContext,
    download_loader,
    load_index_from_storage,
)
from llama_index.llm_predictor.chatgpt import ChatGPTLLMPredictor


def make_query_engine(data, llm, reading, ext):
    if reading:
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿
        storage_context = StorageContext.from_defaults(persist_dir="./storage")
        index = load_index_from_storage(storage_context)
    else:
        prompt_helper = PromptHelper(
            max_input_size=4096, num_output=2048, max_chunk_overlap=20
        )
        llm_predictor = ChatGPTLLMPredictor(llm=llm)
        service_context = ServiceContext.from_defaults(
            llm_predictor=llm_predictor,
            prompt_helper=prompt_helper,
            chunk_size_limit=512,
        )
        if ext == ".pdf":
            PDFReader = download_loader("PDFReader")
            loader = PDFReader()
            documents = loader.load_data(file=data)
        elif ext in [".txt", ".md"]:
            documents = SimpleDirectoryReader(data)
        elif ext == ".pptx":
            PptxReader = download_loader("PptxReader")
            loader = PptxReader()
            documents = loader.load_data(file=data)
        elif ext == ".docx":
            DocxReader = download_loader("DocxReader")
            loader = DocxReader()
            documents = loader.load_data(file=data)
        elif ext in [".png", ".jpeg", ".jpg"]:
            ImageCaptionReader = download_loader("ImageCaptionReader")
            loader = ImageCaptionReader()
            documents = loader.load_data(file=data)

        index = GPTVectorStoreIndex.from_documents(
            documents, service_context=service_context
        )
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä¿å­˜
        # index.storage_context.persist()

    query_engine = index.as_query_engine()

    return query_engine


def chat(text, settings, max_tokens, model):

    messages = [
        {"role": "system", "content": settings},
        {"role": "user", "content": text},
    ]

    try:
        resp = openai.ChatCompletion.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            stream=True,
        )
        return resp
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼:{e}")
        st.stop()


def main():
    if "alltext" not in st.session_state:
        st.session_state["alltext"] = []

    st.write("# ğŸ“šLearnMateAI ")
    st.write("---")
    status_place = st.container()

    with st.sidebar:
        with st.form("settings"):
            model = st.selectbox("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ", ["gpt-3.5-turbo", "gpt-4"])
            inputtext = st.text_input("ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›", help="å¿…é ˆ")
            level = st.selectbox("ãƒ¬ãƒ™ãƒ«ã‚’é¸æŠ", ["åˆå¿ƒè€…", "ä¸­ç´šè€…", "ä¸Šç´šè€…"])
            input_gen_length = st.number_input(
                "ç”Ÿæˆæ–‡å­—æ•°ã‚’å…¥åŠ›", min_value=0, step=100, value=1000, help="0ã«è¨­å®šã™ã‚‹ã¨æŒ‡å®šãªã—ã¨ãªã‚Šã¾ã™ã€‚"
            )
            orginal_file = st.file_uploader("ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã€‚")
            reading = True

            submit = st.form_submit_button("ç”Ÿæˆé–‹å§‹")
        with st.expander("ğŸ“šLearnMateAIã¨ã¯"):
            st.write(
                """
æŒ‡å®šã•ã‚ŒãŸãƒ†ãƒ¼ãƒã¨å¯¾è±¡è€…ã®ãƒ¬ãƒ™ãƒ«ã«æ²¿ã£ãŸè³‡æ–™ã‚’Markdownå½¢å¼ã§ç”Ÿæˆã™ã‚‹AIã§ã™ã€‚  
è‡ªå·±å­¦ç¿’ç”¨ã®è³‡æ–™ä½œæˆã‹ã‚‰ã€ç ”ä¿®è³‡æ–™ä½œæˆã¾ã§å¹…åºƒãå¯¾å¿œã—ã¾ã™ã€‚  

ç”Ÿæˆæ–‡å­—æ•°ã‚’300æ–‡å­—ä»¥å†…ã«æŒ‡å®šã™ã‚‹ã¨æ¦‚è¦èª¬æ˜è³‡æ–™ã‚’ç”Ÿæˆã—ã€ãã‚Œä»¥ä¸Šã‚ã‚‹ã„ã¯0ï¼ˆæŒ‡å®šãªã—ï¼‰ã¨ã™ã‚‹ã¨ç ”ä¿®ã«ä½¿ç”¨ã§ãã‚‹è³‡æ–™â€»ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
â€»ç†è§£åº¦ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®ã‚¯ã‚¤ã‚ºä»˜ã

äº‹å‰ã«ç”¨æ„ã—ãŸè³‡æ–™ã‚’ã‚‚ã¨ã«ã‚¬ã‚¤ãƒ‰ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ï¼ˆç¤¾å†…è³‡æ–™ç­‰ã‚’ã‚‚ã¨ã«æ–°è¦å‚å…¥è€…ã®å—å…¥è³‡æ–™ä½œæˆã‚„ç‹¬è‡ªãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®ä½œæˆãªã©ã«æ´»ç”¨ã§ãã¾ã™ã€‚ï¼‰

"""
            )

    if submit:
        st.session_state["alltext"] = []
        llm = ChatOpenAI(
            temperature=0,
            model_name=model,
            streaming=True,
            max_tokens=2000,
            callback_manager=BaseCallbackManager([StreamlitCallbackHandler()]),
        )

        if orginal_file:
            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                fp = Path(tmp_file.name)
                fp.write_bytes(orginal_file.getvalue())
                query_engine = make_query_engine(
                    tmp_file.name,
                    llm=llm,
                    reading=False,
                    ext=os.path.splitext(orginal_file.name)[1],
                )

        if input_gen_length <= 300:
            gen_rule = f"åˆå­¦è€…ãŒæ¦‚è¦ã‚’æŠŠæ¡ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã®è³‡æ–™ã‚’{input_gen_length}æ–‡å­—ä»¥å†…ã§ä½œæˆã—ã¦ãã ã•ã„"
        else:
            gen_rule = f"{level}ãŒèƒ½åŠ›ã‚’é«˜ã‚ã‚‰ã‚Œã‚‹ç ”ä¿®è³‡æ–™ã‚’{input_gen_length}æ–‡å­—ä»¥å†…ã§ä½œæˆã—ã¦ãã ã•ã„"

        instructions = f"""
ã‚ãªãŸã¯{inputtext}ã«ãŠã‘ã‚‹ãƒ™ãƒ†ãƒ©ãƒ³ã®ç ”ä¿®è¬›å¸«ã§ã™ã€‚
{inputtext}ã«ã¤ã„ã¦ã€{gen_rule}ã€‚
ä½œæˆã«å½“ãŸã£ã¦ã¯ä»¥ä¸‹ã«å³å¯†ã«å¾“ã£ã¦ãã ã•ã„ã€‚
- æŒ‡ç¤ºã®æœ€å¾Œã«[æŒ‡ç¤ºï¼šç¶šãã‚’å‡ºåŠ›]ã¨é€ã‚‰ã‚ŒãŸå ´åˆã¯ã€[æŒ‡ç¤ºï¼šç¶šãã‚’å‡ºåŠ›]ã®å‰ã®æ–‡ç« ã®ç¶šãã‚’å‡ºåŠ›ã™ã‚‹ã€‚
- step by stepã§è¤‡æ•°å›æ¤œè¨ã‚’è¡Œã„ã€ãã®ä¸­ã§ä¸€ç•ªå„ªã‚Œã¦ã„ã‚‹ã¨æ€ã†çµæœã‚’å‡ºåŠ›ã™ã‚‹ã€‚
- ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ãªããã®ã¾ã¾åˆ©ç”¨ã§ãã‚‹ä½“è£ã¨ã™ã‚‹ã€‚
- ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚„ã‚·ã‚§ãƒ«ãªã©ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã™ã‚‹å†…å®¹ã®å ´åˆã¯ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆ©ç”¨ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
- å‡ºåŠ›ã¯Markdownã¨ã™ã‚‹ã€‚å¿…è¦ã«å¿œã˜ã¦summary,detailsãªã©ã®HTMLè¦ç´ ã‚‚çµ„ã¿åˆã‚ã›ã‚‹ã€‚
- å„ç¨®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ç°¡æ½”ã‹ã¤è©³ç´°ã«è¨˜è¼‰ã™ã‚‹ã€‚
- ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¸­ç›¤ã§ã¯ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¿ã‚¤ãƒ ã¨ã—ã¦{inputtext}ã«ã¾ã¤ã‚ã‚‹è±†çŸ¥è­˜ã‚’ç¹”ã‚Šäº¤ãœã‚‹ã€‚
- ç”»åƒã‚„çµµæ–‡å­—ã€ã‚¢ã‚¤ã‚³ãƒ³ç­‰ã‚’ä½¿ç”¨ã—è¦–è¦šçš„ã«èˆˆå‘³ã‚’å¼•ãå·¥å¤«ã‚’è¡Œã†ã€‚
- å›³ã‚„ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã™ã‚‹éš›ã¯Graphvizå½¢å¼ã‚ã‚‹ã„ã¯PlantUMLå½¢å¼ã¨ã™ã‚‹ã€‚
- ç”»åƒã¯base64å½¢å¼ã§å‡ºåŠ›ã™ã‚‹ã€‚
- å„ç¨®æƒ…å ±ã«ã¯å‡ºå…¸ã‚’æ˜è¨˜ã™ã‚‹ã€‚
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã«ç†è§£åº¦ã‚’ç¢ºèªã™ã‚‹ç°¡å˜ãªã‚¯ã‚¤ã‚ºã‚’ä½œæˆã™ã‚‹ã€‚
- ç”Ÿæˆç‰©ä»¥å¤–ã¯å‡ºåŠ›ã—ãªã„ï¼ˆä¾‹ãˆã°ç”Ÿæˆç‰©ã«å¯¾ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã‚„èª¬æ˜ãªã©ï¼‰

    """
        original_instructions = f"ã€€ãƒ«ãƒ¼ãƒ«:{input_gen_length}æ–‡å­—ä»¥å†…ã§å‡ºåŠ›ã€‚Markdownã§å‡ºåŠ›ã€‚æ—¥æœ¬èªã§å‡ºåŠ›ã€‚"

        if inputtext:
            st.session_state["alltext"].append(inputtext)
            text = ""

            with st.spinner(text="ç”Ÿæˆä¸­..."):
                st.write(f"## ãƒ†ãƒ¼ãƒï¼š{inputtext}")
                new_place = st.empty()
                finish_reason = "init"
                completion = ""
                while True:
                    if finish_reason == "init":
                        message = "".join(st.session_state["alltext"])
                    elif finish_reason == "stop":
                        break
                    elif finish_reason == "length":
                        message = "".join(st.session_state["alltext"]) + "[æŒ‡ç¤ºï¼šç¶šãã‚’å‡ºåŠ›]"
                    else:
                        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚finish_reason={finish_reason}")
                        st.stop

                    message = message[0:3500]

                    if orginal_file:
                        query_engine.query(message + original_instructions)
                        break
                    else:
                        completion = chat(
                            text=message,
                            settings=instructions,
                            max_tokens=3500,
                            model=model,
                        )
                        for chunk in completion:
                            finish_reason = chunk["choices"][0].get("finish_reason", "")
                            next = chunk["choices"][0]["delta"].get("content", "")
                            text += next
                            text = text.replace("[æŒ‡ç¤ºï¼šç¶šãã‚’å‡ºåŠ›]", "")
                            new_place.write(text)

                    st.session_state["alltext"].append(text)

            t_delta = datetime.timedelta(hours=9)
            JST = datetime.timezone(t_delta, "JST")
            now = datetime.datetime.now(JST)

            with status_place:
                st.write("### ğŸ‰ç”Ÿæˆå®Œäº†ï¼\n---")
                st.download_button(
                    "ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    file_name=f"LearnMateAI_{now.strftime('%Y%m%d%H%M%S')}.md",
                    data="\n".join(st.session_state["alltext"]),
                    mime="text/plain",
                )
            st.session_state["alltext"] = []


if __name__ == "__main__":
    st.set_page_config(page_title="LearnMateAI", page_icon="ğŸ“š", layout="wide")

    hide_streamlit_style = """
                <style>
                #MainMenu {visibility: hidden;}
                footer {visibility: hidden;}
                </style>
                """
    st.markdown(hide_streamlit_style, unsafe_allow_html=True)

    openai.api_key = st.secrets["OPEN_AI_KEY"]
    os.environ["OPENAI_API_KEY"] = st.secrets["OPEN_AI_KEY"]

    main()
